\documentclass[11pt, letterpaper]{article}
\usepackage[margin=1.5cm]{geometry}
\pagestyle{plain}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage[shortlabels]{enumitem}
\usepackage[makeroom]{cancel}
\usepackage{xcolor}
\usepackage{graphicx}

\begin{document}
\title{Assignment 4\\\normalsize MATH391}
\author{Connor Braun}

\allowdisplaybreaks

\maketitle
\section*{Problem 1}
\subsection*{\normalfont Compute the following for matrices
\begin{align*}
    A=
    \begin{bmatrix}
        1 & 1 & -1& 2\\
        2 & 2 & 4 & 5\\
        1 & -1 & 1 & 7\\
        2 & 3 & 4 & 6
    \end{bmatrix}
    ,\indent B=
    \begin{bmatrix}
    4 & 1 & 1 & 1\\
    1 & 3 & 0 & -1\\
    1 & 0 & 2 & 1\\
    1 & -1 & 1 & 4    
    \end{bmatrix}.
\end{align*}}
\subsection*{a)\normalfont Compute the $LU$ factorization of $A$ with partial pivoting.}
We seek to find permutation matrix $P$, unitary lower triangular matrix $L$ and upper triangular matrix
$U$ such that
\begin{align*}
    PA=LU.
\end{align*}
We proceed with the process of Gaussian elimination with partial pivoting on $A$, noting all row interchanges
between steps, and recording Gaussian multipliers below the main diagonal in {\color{red}red} in place of zeros.
\begin{align*}
    &\begin{bmatrix}
        1 & 1 & -1& 2\\
        2 & 2 & 4 & 5\\
        1 & -1 & 1 & 7\\
        2 & 3 & 4 & 6
    \end{bmatrix}
    \overset{R_1\leftrightarrow R_2}{\longrightarrow}
    \begin{bmatrix}
        2 & 2 & 4 & 5\\
        1 & 1 & -1& 2\\
        1 & -1 & 1 & 7\\
        2 & 3 & 4 & 6
    \end{bmatrix}
    \underset{R_4-\frac{2}{2}R_1}{\overset{R_3 - \frac{1}{2}R_1}{\overset{R_2 - \frac{1}{2}R_1}{\longrightarrow}}}
    \begin{bmatrix}
        2 & 2 & 4 & 5\\
        {\color{red}\frac{1}{2}} & 0 & -3& -\frac{1}{2}\\
        {\color{red}\frac{1}{2}}  & -2 & -1 & \frac{9}{2}\\
        {\color{red}1}  & 1 & 0 & 1
    \end{bmatrix}
    \overset{R_2\leftrightarrow R_3}{\longrightarrow}
    \begin{bmatrix}
        2 & 2 & 4 & 5\\
        {\color{red}\frac{1}{2}}  & -2 & -1 & \frac{9}{2}\\
        {\color{red}\frac{1}{2}} & 0 & -3& -\frac{1}{2}\\
        {\color{red}1}  & 1 & 0 & 1
    \end{bmatrix}
    \dots\\
    &\dots \overset{R_3-\frac{0}{-2}R_2}{\overset{R_4-\frac{1}{-2}R_2}{\longrightarrow}}
    \begin{bmatrix}
        2 & 2 & 4 & 5\\
        {\color{red}\frac{1}{2}}  & -2 & -1 & \frac{9}{2}\\
        {\color{red}\frac{1}{2}} & {\color{red}0} & -3& -\frac{1}{2}\\
        {\color{red}1}  & {\color{red}-\frac{1}{2}} & -\frac{1}{2} & \frac{13}{4}
    \end{bmatrix}
    \overset{R_4-\frac{1}{6}R_3}{\longrightarrow}
    \begin{bmatrix}
        2 & 2 & 4 & 5\\
        {\color{red}\frac{1}{2}}  & -2 & -1 & \frac{9}{2}\\
        {\color{red}\frac{1}{2}} & {\color{red}0} & -3& -\frac{1}{2}\\
        {\color{red}1}  & {\color{red}-\frac{1}{2}} & {\color{red}\frac{1}{6}} & \frac{10}{3}
    \end{bmatrix}
\end{align*}
We now apply precisely the same row interchanges on $I_4$ as we did on the system to find $P$.
\begin{align*}
    \begin{bmatrix}
        1 & 0 & 0 & 0\\
        0 & 1 & 0 & 0\\
        0 & 0 & 1 & 0\\
        0 & 0 & 0 & 1
    \end{bmatrix}
    \overset{R_1\leftrightarrow R_2}{\longrightarrow}
    \begin{bmatrix}
        0 & 1 & 0 & 0\\
        1 & 0 & 0 & 0\\
        0 & 0 & 1 & 0\\
        0 & 0 & 0 & 1
    \end{bmatrix}
    \overset{R_2\leftrightarrow R_3}{\longrightarrow}
    \begin{bmatrix}
        0 & 1 & 0 & 0\\
        0 & 0 & 1 & 0\\
        1 & 0 & 0 & 0\\
        0 & 0 & 0 & 1
    \end{bmatrix}
    =P.
\end{align*}
Finally, $L$ and $U$ are given by separating our system with all Gaussian multipliers into a unitary 
lower triangular component and an upper triangular component.
\begin{align*}
    \begin{bmatrix}
        2 & 2 & 4 & 5\\
        {\color{red}\frac{1}{2}}  & -2 & -1 & \frac{9}{2}\\
        {\color{red}\frac{1}{2}} & {\color{red}0} & -3& -\frac{1}{2}\\
        {\color{red}1}  & {\color{red}-\frac{1}{2}} & {\color{red}\frac{1}{6}} & \frac{10}{3}
    \end{bmatrix}
    \Rightarrow L=
    \begin{bmatrix}
        1 & 0 & 0 & 0\\
        \frac{1}{2} & 1 & 0 & 0\\
        \frac{1}{2} & 0 & 1 & 0\\
        1 & -\frac{1}{2} & \frac{1}{6} & 1
    \end{bmatrix}
    ,\indent U=
    \begin{bmatrix}
        2 & 2 & 4 & 5\\
        0 & -2 & -1 & \frac{9}{2}\\
        0 & 0 & -3 & -\frac{1}{2}\\
        0 & 0 & 0 & \frac{10}{3}
    \end{bmatrix}
\end{align*}
Where matrices $P$, $A$, $L$ and $U$ satisfy the $LU$-factorization equation
\begin{align*}
    PA=LU.
\end{align*}
\subsection*{b)\normalfont Compute the $LDL^T$ factorization of $B$.}
We seek to find diagonal matrix $D$ and lower unitary triangular matrix $L$ such that
\begin{align*}
    B=LDL^T
\end{align*}
Furthermore, $D$ is determined by the upper triangular matrix $U$ satisfying the $LU$ decomposition of $B$
\begin{align*}
    B=LU
\end{align*}
where $diag(D)=diag(U)$. We begin with the process of Gaussian elimination without pivoting, recording Gaussian
multipliers below the main diagonal in {\color{red}red} in place of zeros.
\begin{align*}
    \begin{bmatrix}
        4 & 1 & 1 & 1\\
        1 & 3 & 0 & -1\\
        1 & 0 & 2 & 1\\
        1 & -1 & 1 & 4    
    \end{bmatrix}
    \overset{R_2-\frac{1}{4}R_1}{\overset{R_3-\frac{1}{4}R_1}{\underset{R_4-\frac{1}{4}R_1}{\longrightarrow}}}
    \begin{bmatrix}
        4 & 1 & 1 & 1\\
        {\color{red}\frac{1}{4}} & \frac{11}{4} & -\frac{1}{4} & -\frac{5}{4}\\
        {\color{red}\frac{1}{4}}  & -\frac{1}{4} & \frac{7}{4} & \frac{3}{4}\\
        {\color{red}\frac{1}{4}}  & -\frac{5}{4} & \frac{3}{4} & \frac{15}{4}    
    \end{bmatrix}
    \overset{R_3-\frac{-1}{11}R_2}{\overset{R_4-\frac{-5}{11}R_2}{\longrightarrow}}
    \begin{bmatrix}
        4 & 1 & 1 & 1\\
        {\color{red}\frac{1}{4}} & \frac{11}{4} & -\frac{1}{4} & -\frac{5}{4}\\
        {\color{red}\frac{1}{4}}  & {\color{red}-\frac{1}{11}} & \frac{76}{44} & \frac{28}{44}\\
        {\color{red}\frac{1}{4}}  & {\color{red}-\frac{5}{11}} & \frac{28}{44} & \frac{140}{44}    
    \end{bmatrix}
    \overset{R_4-\frac{7}{19}R_3}{\longrightarrow}
    \begin{bmatrix}
        4 & 1 & 1 & 1\\
        {\color{red}\frac{1}{4}} & \frac{11}{4} & -\frac{1}{4} & -\frac{5}{4}\\
        {\color{red}\frac{1}{4}}  & {\color{red}-\frac{1}{11}} & \frac{76}{44} & \frac{28}{44}\\
        {\color{red}\frac{1}{4}}  & {\color{red}-\frac{5}{11}} & {\color{red}\frac{7}{19}} & \frac{2464}{836}    
    \end{bmatrix}
\end{align*}
Now, as was the case in 1.a, $L$ and $U$ are given by separating our system with all Gaussian multipliers
into a unitary lower triangular component and an upper triangular component.
\begin{align*}
    \begin{bmatrix}
        4 & 1 & 1 & 1\\
        {\color{red}\frac{1}{4}} & \frac{11}{4} & -\frac{1}{4} & -\frac{5}{4}\\
        {\color{red}\frac{1}{4}}  & {\color{red}-\frac{1}{11}} & \frac{76}{44} & \frac{28}{44}\\
        {\color{red}\frac{1}{4}}  & {\color{red}-\frac{5}{11}} & {\color{red}\frac{7}{19}} & \frac{2464}{836}    
    \end{bmatrix}
    \Rightarrow L=
    \begin{bmatrix}
        1 & 0 & 0 & 0\\
        \frac{1}{4} & 1 & 0 & 0\\
        \frac{1}{4}  & -\frac{1}{11} & 1 & 0\\
        \frac{1}{4}  & -\frac{5}{11} & \frac{7}{19} & 1   
    \end{bmatrix}
    ,\indent U=
    \begin{bmatrix}
        4 & 1 & 1 & 1\\
        0 & \frac{11}{4} & -\frac{1}{4} & -\frac{5}{4}\\
        0 & 0 & \frac{76}{44} & \frac{28}{44}\\
        0  & 0 & 0 & \frac{2464}{836}    
    \end{bmatrix}
    =
    \begin{bmatrix}
        4 & 1 & 1 & 1\\
        0 & \frac{11}{4} & -\frac{1}{4} & -\frac{5}{4}\\
        0 & 0 & \frac{19}{11} & \frac{7}{11}\\
        0  & 0 & 0 & \frac{56}{19}    
    \end{bmatrix}
\end{align*}
But since $diag(U)=diag(D)$ and $D$ is diagonal, we have that
\begin{align*}
    D=
    \begin{bmatrix}
        4 & 0 & 0 & 0\\
        0 & \frac{11}{4} & 0 & 0\\
        0 & 0 & \frac{19}{11} & 0\\
        0 & 0 & 0 & \frac{56}{19}
    \end{bmatrix}
\end{align*}
Where now the matrices $L$ and $D$ satisfy the factorization of symmetric matrix $B$ given by
\begin{align*}
    B=LDL^T
\end{align*}
\subsection*{c)\normalfont Compute the Cholesky factorization of $B$.}
    We seek the lower triangular matrix $G$ with positive entries on the main diagonal such that
    \begin{align*}
        B=GG^T.
    \end{align*}
    Such a matrix is guaranteed to exist only when $B$ is positive definite. Let $\eta_i$, $0\leq i\leq 4$
    be the 4 principle minors of $B$. Then, since $B$ is symmetric, $B$ is positive definite if and only 
    if $\eta_i>0$, $0\leq i\leq 4$. Then, we proceed by computing the principle minors of $B$. 
    \begin{align*}
        &\eta_1=4>0\\
        &\eta_2=
        \begin{vmatrix}
            4 & 1\\
            1 & 3
        \end{vmatrix}
        =12-1=11>0\\
        &\eta_3=
        \begin{vmatrix}
            4 & 1 & 1\\
            1 & 3 & 0\\
            1 & 0 & 2
        \end{vmatrix}
        =4
        \begin{vmatrix}
            3 & 0\\
            0 & 2
        \end{vmatrix}
        -
        \begin{vmatrix}
            1 & 0\\
            1 & 2
        \end{vmatrix}
        +
        \begin{vmatrix}
            1 & 3\\
            1 & 0
        \end{vmatrix}
        =24-2-3=19>0\\
        &\eta_4=
        \begin{vmatrix}
            4 & 1 & 1 & 1\\
            1 & 3 & 0 & -1\\
            1 & 0 & 2 & 1\\
            1 & -1 & 1 & 4    
        \end{vmatrix}
        =4
        \begin{vmatrix}
            3 & 0 & -1\\
            0 & 2 & 1\\
            -1 & 1 & 4
        \end{vmatrix}
        -
        \begin{vmatrix}
            1 & 0 & -1\\
            1 & 2 & 1\\
            1 & 1 & 4
        \end{vmatrix}
        +
        \begin{vmatrix}
            1 & 3 & -1\\
            1 & 0 & 1\\
            1 & -1 & 4
        \end{vmatrix}
        -
        \begin{vmatrix}
            1 & 3 & 0\\
            1 & 0 & 2\\
            1 & -1 & 1
        \end{vmatrix}\\
        &\indent =4\left(3\begin{vmatrix}
            2 & 1\\
            1 & 4
        \end{vmatrix}
        -
        \begin{vmatrix}
            0 & 2\\
            -1 & 1
        \end{vmatrix}\right)
        -\left(\begin{vmatrix}
            2 & 1\\
            1 & 4
        \end{vmatrix}
        -
        \begin{vmatrix}
            1 & 2\\
            1 & 1
        \end{vmatrix}
        \right)
        +\left(\begin{vmatrix}
                0 & 1\\
                -1 & 4
            \end{vmatrix}
            -3
            \begin{vmatrix}
                1 & 1\\
                1 & 4
            \end{vmatrix}
            -
            \begin{vmatrix}
                1 & 0\\
                1 & -1
            \end{vmatrix}
        \right)
        -\left(\begin{vmatrix}
                0 & 2\\
                -1 & 1
            \end{vmatrix}
            -3
            \begin{vmatrix}
                1 & 2\\
                1 & 1
            \end{vmatrix}
        \right)\\
        &\indent= 4(3(7)-2)-(7+1)+(1-3(3)+1)-(2-3(-1))\\
        &\indent =56>0
    \end{align*}
    So, since all positive minors are strictly greater than zero, $B$ is positive definite. We can then use 
    matrices $L$ and $D$ as computed in 1.b to find $G$. Let $\tilde{D}$ be the diagonal matrix obtained
    by taking the square root of each element in $D$. Then we have
    \begin{align*}
        G=L\tilde{D}.
    \end{align*}
    We can then compute $G$ directly.
    \begin{align*}
        G=
        \begin{bmatrix}
            1 & 0 & 0 & 0\\
            \frac{1}{4} & 1 & 0 & 0\\
            \frac{1}{4}  & -\frac{1}{11} & 1 & 0\\
            \frac{1}{4}  & -\frac{5}{11} & \frac{7}{19} & 1   
        \end{bmatrix}
        \begin{bmatrix}
            2 & 0 & 0 & 0\\
            0 & \frac{\sqrt{11}}{2} & 0 & 0\\
            0 & 0 & \frac{\sqrt{19}}{\sqrt{11}} & 0\\
            0 & 0 & 0 & \frac{2\sqrt{14}}{\sqrt{19}}
        \end{bmatrix}
        =
        \begin{bmatrix}
            2 & 0 & 0 & 0\\
            \frac{1}{2} & \frac{\sqrt{11}}{2} & 0 & 0\\
            \frac{1}{2} & -\frac{\sqrt{11}}{22} & \frac{\sqrt{19}}{\sqrt{11}} & 0\\
            \frac{1}{2} & -\frac{5\sqrt{11}}{22} & \frac{7\sqrt{19}}{19\sqrt{11}} & \frac{2\sqrt{14}}{\sqrt{19}}
        \end{bmatrix}
    \end{align*}
    Where $G$ satisfies the Cholesky decomposition for matrix $B$ given by
    \begin{align*}
        B=GG^T
    \end{align*}
\section*{Problem 2}
\subsection*{a)\normalfont Write the algorithm that computes matrices $L$ and $U$ such that for tridiagonal matrix
$A$ we can write $A=LU$.}
Furthermore, the question specifies a form of $L$ which precludes partial pivoting. Namely, that $L$ is
a unitary lower triangular matrix of all zeros except for on the main diagonal and first subdiagonal. Partial 
pivoting could result in nonzero entries of $L$ beneath the first subdiagonal, so we exclude it from
our algorithm. We can induce an algorithm by considering the case of $3\times 3$ tridiagonal matrix $B$ given by
\begin{align*}
    \begin{bmatrix}
        a_1 & c_1 & 0\\
        b_2 & a_2 & c_2\\
        0 & b_3 & a_3
    \end{bmatrix}
\end{align*}
where all of $a_i$, $b_i$ and $c_i$ are real numbers for $i\in\mathbb{N}$, $1\leq i \leq 3$. Simply applying
the Gaussian elimination algorithm and recording Gaussian multipliers below the main diagonal in {\color{red}red}
in place of zeros, we have
\begin{align*}
    \begin{bmatrix}
        a_1 & c_1 & 0\\
        b_2 & a_2 & c_2\\
        0 & b_3 & a_3
    \end{bmatrix}
    \overset{R_2-\frac{b_2}{a_1}R_1}{\overset{R_3-0R_1}{\longrightarrow}}
    \begin{bmatrix}
        a_1 & c_1 & 0\\
        {\color{red}\frac{b_2}{a_1}} & a_2' & c_2\\
        {\color{red}0} & b_3 & a_3
    \end{bmatrix}
    \overset{R_3-\frac{b_3}{a_2'}}{\longrightarrow}
    \begin{bmatrix}
        a_1 & c_1 & 0\\
        {\color{red}\frac{b_2}{a_1}} & a_2' & c_2\\
        {\color{red}0} & {\color{red}\frac{b_3}{a_2'}} & a_3'
    \end{bmatrix}
\end{align*}
Where now we have the $LU$ factorization of $B$ such that $B=L_BU_B$ is given by
\begin{align*}
    L_B=
    \begin{bmatrix}
        1 & 0 & 0\\
        \frac{b_2}{a_1} & 1 & 0\\
        0 & \frac{b_3}{a_2'} & 1
    \end{bmatrix}
    ,\indent U_B=
    \begin{bmatrix}
        a_1 & c_1 & 0\\
        0 & a_2' & c_2\\
        0 & 0 & a_3'
    \end{bmatrix}
\end{align*}
Which is a solution in precisely the form required. However, we require that $a_1\neq 0$ and $a_j'\neq 0$,
$j\in\mathbb{N}$, $2\leq j \leq 3$. Assuming this to be the case, we can see that
\begin{align*}
    a_1&=a_1\\
    a_2'&=a_2-\frac{b_2}{a_1}c_1\\
    a_3'&=a_3-\frac{b_3}{a_2'}c_2
\end{align*}
Which we can write as the recursive formula
\begin{align*}
    a_1&=a_1=a_1'\\
    a_j'&=a_j-\frac{b_j}{a_{j - 1}'}c_{j - 1},\indent 2\leq j\leq 3
\end{align*}
Then, for the general $n\times n$ tridiagonal matrix given by
\begin{align*}
    A=
    \begin{bmatrix}
        u_1 & w_1 & 0 & & & \dots & \dots & \dots & & 0\\
        v_2 & u_2 & w_2 & 0 & & & & & & \\
        0 & v_3 & u_3 & w_3 & 0 & & & & & \\
        \vdots & \ddots & \ddots & \ddots & \ddots & \ddots & & & & \vdots\\
        \vdots & &\ddots & \ddots & \ddots & \ddots & \ddots & & & \vdots\\
        \vdots & & &\ddots & \ddots & \ddots & \ddots & \ddots & & \vdots\\
        \vdots & & & &\ddots & \ddots & \ddots & \ddots & \ddots & \vdots\\
        & & & & & 0 & v_{n-2} & u_{n-2} & w_{n-2} & 0\\
        & & & & & & 0 & v_{n-1} & u_{n-1} & w_{n-1}\\
        0 & & & & & & & 0 & v_n & u_n
    \end{bmatrix}
\end{align*}
where we seek unitary lower triangular matrix $L$ and upper triangular matrix $U$ given by
\begin{align*}
    L=
    \begin{bmatrix}
        1 & 0 & & & \dots & \dots & 0\\
        l_2 & 1 & 0 & & & & &\\
        0 & l_3 & 1 & 0 & & & &\\
        \vdots & \ddots & \ddots & \ddots & \ddots & & \vdots\\
        \vdots & & \ddots & \ddots & \ddots & \ddots & \vdots\\
        & & & 0 & l_{n-1} & 1 & 0\\
        0 & & & & 0 & l_n & 1
    \end{bmatrix}
    ,\indent U=
    \begin{bmatrix}
        z_1 & w_1 & & & \dots & \dots & 0\\
        0 & z_2 & w_2 & & & & &\\
        0 & 0 & z_3 & w_3 & & & &\\
        \vdots & \ddots & \ddots & \ddots & \ddots & & \vdots\\
        \vdots & & \ddots & \ddots & \ddots & \ddots & \vdots\\
        & & & 0 & 0 & z_{n-1} & w_{n-1}\\
        0 & & & & 0 & 0 & z_n
    \end{bmatrix}
\end{align*}
which satisfies the $LU$ decomposition of $A$ given by
\begin{align*}
    A=LU
\end{align*}
we do so by the algorithm
\begin{align*}
    \begin{cases}
        z_1&=u_1\\
        z_i&=u_i-\frac{v_i}{z_{i-1}}w_{j-1},\indent 2\leq i\leq n\\
        l_j&=\frac{v_j}{z_{i-1}},\indent 2\leq j\leq n
    \end{cases}        
\end{align*}
Importantly, we note that this algorithm fails if any of $z_1$, $z_2$, \dots, $z_{n-1}$ is zero.
\subsection*{b)\normalfont Write Matlab function $TriLU()$ which computes $LU$ factorization of an arbitrary
tridiagonal matrix according to the algorithm proposed in 2.a and the additional specifications in the 
assignment.}
\begin{verbatim}
    function [l, u] = TriLU(a, b, c, n)
        l = zeros(n - 1, 1);
        u = zeros(n, 1);
        u(1) = a(1);
    
        for i = 2:(n)
            if i ~= n
                assert(u(i - 1) ~= 0, sprintf("TriLU failure; pivot %d is 0", i - 1))
            end
            gauss_multiplier = b(i - 1)/u(i - 1);
        
            l(i - 1) = gauss_multiplier;
            u(i) = a(i) - gauss_multiplier*c(i - 1);
        end
    end
\end{verbatim}
\section*{Problem 3}
\subsection*{\normalfont Show that if $A=(a_{ij})_{1\leq i,j\leq n}$ is an $n\times n$ matrix, then
\begin{align*}
    \||A|\|_{\infty}=\underset{1\leq i\leq n}{max}\sum_{j=1}^n|a_{ij}|.
\end{align*}}
{\bf Proof}. Let $x\in\mathbb{R}^n$, where $x=[x_1$ $x_2$ $\dots$ $x_n]^T$. Then we proceed by first stating the definition of a matrix norm.
\begin{align*}
    |\|A\||_\infty&=\underset{x\neq 0}{\underset{x\in\mathbb{R}^n}{max}}\frac{\|Ax\|_\infty}{\|x\|_\infty}\\
\end{align*}
Let $x_m=\underset{1\leq i\leq n}{max}|x_i|$ where since $x\neq 0$, $0<x_m$. Then, we can proceed to expand the system $Ax$ and simplify
by the definition of the infinity norm.
\begin{align*}
    \||A|\|_\infty&=\underset{x\neq 0}{\underset{x\in\mathbb{R}^n}{max}}\left(\frac{\left\Vert
    \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n}\\
        a_{21} & a_{22} & \dots & a_{2n}\\
        \vdots & \vdots & \ddots & \vdots\\
        a_{n1} & a_{n2} & \dots & a_{nn}
    \end{bmatrix}
    \begin{bmatrix}
        x_1\\
        x_2\\
        \vdots\\
        x_n
    \end{bmatrix}
    \right\Vert_\infty}{\underset{1\leq i\leq n}{max}|x_i|}\right)
    =\underset{x\neq 0}{\underset{x\in\mathbb{R}^n}{max}}\left(\frac{\underset{1\leq i\leq n}{max}\left|\sum\limits_{j=1}^na_{ij}x_j\right|}{x_m}\right)\\
    &=\underset{x\neq 0}{\underset{x\in\mathbb{R}^n}{max}}\left(\underset{1\leq i\leq n}{max}\left|\sum\limits_{j=1}^na_{ij}\frac{x_j}{x_m}\right|\right)
\end{align*}
We know that the maximum possible value of $\frac{x_j}{x_m}$, $j=1$, $2$, $\dots$, $n$ is 1, which occurs
when both $|x_j|=x_m$ and $x_j$ has the same sign as $a_{ij}$. Hence the vector $x\in\mathbb{R}^n$ which
maximizes this sum is that for which $\forall_j$, $\frac{x_j}{x_m}=\pm 1$ and $a_{ij}x_j\geq 0$. Letting 
$x$ have these properties, we get that $\forall_j$, $|a_{ij}|=a_{ij}\frac{x_j}{x_m}$ so the above 
simplifies to the equality of interest:
\begin{align*}
    \||A|\|_\infty=\underset{1\leq i\leq n}{max}\sum_{j=1}^n|a_{ij}|.
\end{align*}
\section*{Problem 4}
\subsection*{\normalfont Write Matlab functions $myJacobi()$ and $myGaussSeidel$ which iteratively
approximate $x\in\mathbb{R}^n$ in linear system of equations $Ax=b$, where $A$ is an $n\times n$ matrix
and $b\in\mathbb{R}^n$. Functions must meet specifications in assignment.}
\begin{verbatim}
    function [x_n, n] = myJacobi(A, b, x_0, nmax, tol)
        dim = numel(b);
        x_n = x_0;
        for n = 1:nmax
            for i = 1:dim
                row_operator = 0;
                for j = 1:dim
                    if j ~= i
                        row_operator = row_operator + A(i, j)*x_0(j);
                    end
                end
                x_n(i) = (b(i) - row_operator)/A(i, i);
            end
            if norm(x_n - x_0)/norm(x_n) < tol
                disp("myJacobi return condition: ||x_k - x_k-1||/||x_k|| < tol")
                return
            end
            x_0 = x_n;
        end
        disp("myJacobi did not achieve convergence criterion within iteration allowance")
    end

    function [x_n, n] = myGaussSeidel(A, b, x_0, nmax, tol)
        dim = numel(b);
        x_n = x_0;
        for n = 1:nmax
            for i = 1:dim
                row_operator = 0;
                for j = 1:dim
                    if j ~= i
                        row_operator = row_operator + A(i, j)*x_n(j);
                    end
                end
                x_n(i) = (b(i) - row_operator)/A(i, i);
            end
            if norm(x_n - x_0)/norm(x_n) < tol
            disp("myGaussSeidel return condition: ||x_k - x_k-1||/||x_k|| < tol")
            return
            end
            x_0 = x_n;
        end
        disp("myGaussSeidel did not achieve convergence criterion within iteration allowance")
    end
\end{verbatim}
\section*{Problem 5}
\subsection*{\normalfont The Jacobi and Gauss-Seidel algorithms applied to linear system $Ax=b$ 
for $x,b\in\mathbb{R}^n$, $A$ an $n\times n$ matrix, read
\begin{align*}
    \begin{cases}
        x^{(0)}={\bf 0}\\
        x^{(k)}=B_Jx^{(k - 1)} + c
    \end{cases}
    \begin{cases}
        x^{(0)}={\bf 0}\\
        x^{(k)}=B_{GS}x^{(k - 1)} + d
    \end{cases}
\end{align*}
for some $c,d\in\mathbb{R}^n$.
}
\subsection*{a)\normalfont Suppose that 
\begin{align*}
    A=
    \begin{bmatrix}
        2 & -1 & 1\\
        2 & 2 & 2\\
        -1 & -1 & 2
    \end{bmatrix}  
    ,\indent b=
    \begin{bmatrix}
        -1\\
        4\\
        -5
    \end{bmatrix} 
\end{align*}
}
\subsubsection*{i)\normalfont Find matrices $B_J$ and $B_{GS}$.}
By definition of the Jacobi and Gauss-Seidel methods, we have that
\begin{align*}
    B_J=(I-D^{-1}A),\indent B_{GS}=(I-L^{-1}A).
\end{align*}
Where $D\in\mathbb{R}^{n\times n}$ is the diagonal component of $A$, and $L\in\mathbb{R}^{n\times n}$
is the lower triangular component of $A$, including the main diagonal. Then we compute $B_J$ and $B_{GS}$
directly.
\begin{align*}
    B_J&=
    \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0\\
        0 & 0 & 1
    \end{bmatrix}
    -
    \begin{bmatrix}
        \frac{1}{2} & 0 & 0\\
        0 & \frac{1}{2} & 0\\
        0 & 0 & \frac{1}{2}
    \end{bmatrix}
    \begin{bmatrix}
        2 & -1 & 1\\
        2 & 2 & 2\\
        -1 & -1 & 2
    \end{bmatrix}\\
    &=
    \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0\\
        0 & 0 & 1
    \end{bmatrix}
    -
    \begin{bmatrix}
        1 & -\frac{1}{2} & \frac{1}{2}\\
        1 & 1 & 1\\
        -\frac{1}{2} & -\frac{1}{2} & 1
    \end{bmatrix}\\
    &=
    \begin{bmatrix}
        0 & \frac{1}{2} & -\frac{1}{2}\\
        -1 & 0 & -1\\
        \frac{1}{2} & \frac{1}{2} & 0
    \end{bmatrix}\\
    L^{-1}&=
    \begin{bmatrix}
        2 & 0 & 0\\
        2 & 2 & 0\\
        -1 & -1 & 2
    \end{bmatrix}^{-1}
    =
    \begin{bmatrix}
        \frac{1}{2} & 0 & 0\\
        -\frac{1}{2} & \frac{1}{2} & 0\\
        0 & \frac{1}{4} & \frac{1}{2} 
    \end{bmatrix}\\
    B_{GS}&=
    \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0\\
        0 & 0 & 1
    \end{bmatrix}
    -
    \begin{bmatrix}
        \frac{1}{2} & 0 & 0\\
        -\frac{1}{2} & \frac{1}{2} & 0\\
        0 & \frac{1}{4} & \frac{1}{2} 
    \end{bmatrix}
    \begin{bmatrix}
        2 & -1 & 1\\
        2 & 2 & 2\\
        -1 & -1 & 2
    \end{bmatrix}\\
    &=
    \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0\\
        0 & 0 & 1
    \end{bmatrix}
    -
    \begin{bmatrix}
        1 & -\frac{1}{2} & \frac{1}{2}\\
        0 & \frac{3}{2} & \frac{1}{2}\\
        0 & 0 & \frac{3}{2}
    \end{bmatrix}\\
    &=
    \begin{bmatrix}
        0 & \frac{1}{2} & -\frac{1}{2}\\
        0 & -\frac{1}{2} & -\frac{1}{2}\\
        0 & 0 & -\frac{1}{2}
    \end{bmatrix}\\
\end{align*}
\subsubsection*{ii)\normalfont Show that $\rho\left(B_J\right)=\frac{\sqrt{5}}{2}$ and $\rho\left(B_{GS}\right)=\frac{1}{2}$.}
Since for $A\in\mathbb{R}^{n\times n}$, $\rho(A)=\underset{1\leq j\leq n}{max}|\lambda_j|$, we first 
seek the eigenvalues of $B_J$ and $B_{GS}$ in order to compute the spectral radius. 
\begin{align*}
    C_{B_J}(\lambda)&=
    \begin{vmatrix}
        -\lambda & \frac{1}{2} & -\frac{1}{2}\\
        -1 & -\lambda & -1\\
        \frac{1}{2} & \frac{1}{2} & -\lambda 
    \end{vmatrix}
    =-\lambda
    \begin{vmatrix}
        -\lambda & -1\\
        \frac{1}{2} & -\lambda
    \end{vmatrix}
    -\frac{1}{2}
    \begin{vmatrix}
        -1 & -1\\
        \frac{1}{2} & -\lambda
    \end{vmatrix}
    -\frac{1}{2}
    \begin{vmatrix}
        -1 & -\lambda\\
        \frac{1}{2} & \frac{1}{2}
    \end{vmatrix}\\
    &=-\lambda\left(\lambda^2+\frac{1}{2}\right)-\frac{1}{2}\left(\lambda + \frac{1}{2}\right)-\frac{1}{2}\left(\frac{\lambda}{2}-\frac{1}{2}\right)\\
    &=-\lambda^3-\frac{\lambda}{2}-\frac{\lambda}{2} -\frac{1}{4} -\frac{\lambda}{4}+\frac{1}{4}\\
    &=-\lambda^3-\frac{5}{4}\lambda\\
    &=-\lambda\left(\lambda^2+\frac{5}{4}\right)\Rightarrow \lambda_1=0,\indent \lambda_{2,3}=\frac{\pm\sqrt{-5}}{2}=\pm\frac{\sqrt{5}}{2}i\\
    &\Rightarrow|\lambda_1|<|\lambda_2|=|\lambda_3|=\sqrt{\frac{5}{4}}=\frac{\sqrt{5}}{2}=\rho(B_J)\\
    C_{B_{GS}}(\lambda)&=
    \begin{vmatrix}
        -\lambda & \frac{1}{2} & -\frac{1}{2}\\
        0 & -\frac{1}{2}-\lambda & -\frac{1}{2}\\
        0 & 0 & -\frac{1}{2}-\lambda
    \end{vmatrix}
    =-\lambda
    \begin{vmatrix}
        -\frac{1}{2}-\lambda & -\frac{1}{2}\\
        0 & -\frac{1}{2}-\lambda
    \end{vmatrix}
    =-\lambda\left(-\frac{1}{2}-\lambda\right)\left(-\frac{1}{2}-\lambda\right)=-\lambda\left(\frac{1}{2}+\lambda\right)\left(\frac{1}{2}+\lambda\right)\\
    &\Rightarrow \lambda_1=0,\indent\lambda_2=\lambda_3=-\frac{1}{2}\\
    &\Rightarrow |\lambda_1|<|\lambda_2|=|\lambda_3|=\frac{1}{2}=\rho(B_{GS})
\end{align*}
\subsubsection*{iii)\normalfont Apply Jacobi and Gauss-Seidel to the system with $x^{(0)}={\bf0}$ and 25 iterations.
Comment on the findings. The exact solution is $x=[1$ $2$ $-1]^T$.}
Using the Matlab functions designed for problem 4, we attain the following results for the system $Ax=b$:
\begin{align*}
    \text{$myJacobi()$: }x^{(25)}=
    \begin{bmatrix}
        -20.8279\\
        2.0000\\
        -22.8279
    \end{bmatrix}
    ,\indent \text{$myGaussSeidel()$: }x^{(25)}=
    \begin{bmatrix}
        1.0000\\
        2.0000\\
        -1.0000
    \end{bmatrix}\\
    \text{$myJacobi()$: }\|x-x^{(25)}\|=30.8693
    ,\indent\text{$myGaussSeidel()$: }\|x-x^{(25)}\|=0
\end{align*}
The Jacobi algorithm fails to converge for the system $Ax=b$ within 25 iterations while the Gauss-Seidel 
algorithm converges handily. Increasing the number of iterations does not improve the outcome:
\begin{align*}
    \text{$myJacobi()$: }x^{(1000)}=
    \begin{bmatrix}
        -1.7106\times10^{48}\\
        -6.8425\times10^{48}\\
        1.7106\times10^{48}
    \end{bmatrix}
\end{align*}
In the text by Epperson (p. 463) we have the following theorem [1]:\\\\
{\bf Theorem. }Let  $A\in\mathbb{R}^{n\times n}$. Let $T=M^{-1}N$, where $A=M-N$. Then 
\[(*)\indent\indent x^{(k+1)}=M^{-1}Nx^{(k)}+M^{-1}b\]
converges for all initial guesses $x^{(0)}$ if and only if $\rho(T)<1$.\\\\
The equation $(*)$, has the form of the Jacobi and Gauss-Seidel algorithms when $M^{-1}N=B_J$ and $M^{-1}N=B_{GS}$
respectively. However, we have that $\rho(B_J)=\frac{\sqrt{5}}{2}>1$, so the Jacobi algorithm is not 
guaranteed to converge for all choices of $x^{(0)}$. On the other hand, $\rho(B_{GS})=\frac{1}{2}<1$, so the Gauss-Seidel
algorithm is guaranteed to converge for all choices of $x^{(0)}$ under these conditions. This result
explains the observed behavior of both algorithms for the current system $Ax=b$.
\subsection*{b) \normalfont Suppose that
\begin{align*}
    A=
    \begin{bmatrix}
        1 & 2 & -2\\
        1 & 1 & 1\\
        2 & 2 & 1
    \end{bmatrix}
    ,\indent b=
    \begin{bmatrix}
        7\\
        2\\
        5
    \end{bmatrix}
\end{align*}
}
\subsubsection*{i)\normalfont Find matrices $B_J$ and $B_{GS}$.}
By definition of the Jacobi and Gauss-Seidel methods, we have that
\begin{align*}
    B_J=(I-D^{-1}A),\indent B_{GS}=(I-L^{-1}A).
\end{align*}
Where $D\in\mathbb{R}^{n\times n}$ is the diagonal component of $A$, and $L\in\mathbb{R}^{n\times n}$
is the lower triangular component of $A$, including the main diagonal. Then we compute $B_J$ and $B_{GS}$
directly.
\begin{align*}
    B_J&=
    \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0\\
        0 & 0 & 1
    \end{bmatrix}
    -
    \begin{bmatrix}
        1 & 0 & 0\\
        0 & 1 & 0\\
        0 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
        1 & 2 & -2\\
        1 & 1 & 1\\
        2 & 2 & 1
    \end{bmatrix}\\
    &=
    \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0\\
        0 & 0 & 1
    \end{bmatrix}
    -
    \begin{bmatrix}
        1 & 2 & -2\\
        1 & 1 & 1\\
        2 & 2 & 1
    \end{bmatrix}\\
    &=
    \begin{bmatrix}
        0 & -2 & 2\\
        -1 & 0 & -1\\
        -2 & -2 & 0
    \end{bmatrix}\\
    L^{-1}&=
    \begin{bmatrix}
        1 & 0 & 0\\
        1 & 1 & 0\\
        2 & 2 & 1
    \end{bmatrix}^{-1}
    =
    \begin{bmatrix}
        1 & 0 & 0\\
        -1 & 1 & 0\\
        0 & -2 & 1 
    \end{bmatrix}\\
    B_{GS}&=
    \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0\\
        0 & 0 & 1
    \end{bmatrix}
    -
    \begin{bmatrix}
        1 & 0 & 0\\
        -1 & 1 & 0\\
        0 & -2 & 1 
    \end{bmatrix}
    \begin{bmatrix}
        1 & 2 & -2\\
        1 & 1 & 1\\
        2 & 2 & 1
    \end{bmatrix}\\
    &=
    \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0\\
        0 & 0 & 1
    \end{bmatrix}
    -
    \begin{bmatrix}
        1 & 2 & -2\\
        0 & -1 & 3\\
        0 & 0 & -1
    \end{bmatrix}\\
    &=
    \begin{bmatrix}
        0 & -2 & 2\\
        0 & 2 & -3\\
        0 & 0 & 2
    \end{bmatrix}
\end{align*}
\subsubsection*{ii)\normalfont Show that $\rho\left(B_J\right)=0$ and $\rho\left(B_{GS}\right)=2$.}
Since for $A\in\mathbb{R}^{n\times n}$, $\rho(A)=\underset{1\leq j\leq n}{max}|\lambda_j|$, we first 
seek the eigenvalues of $B_J$ and $B_{GS}$ in order to compute the spectral radius. 
\begin{align*}
    C_{B_J}(\lambda)&=
    \begin{vmatrix}
        -\lambda & -2 & 2\\
        -1 & -\lambda & -1\\
        -2 & -2 & -\lambda 
    \end{vmatrix}
    =-\lambda
    \begin{vmatrix}
        -\lambda & -1\\
        -2 & -\lambda
    \end{vmatrix}
    +2
    \begin{vmatrix}
        -1 & -1\\
        -2 & -\lambda
    \end{vmatrix}
    +2
    \begin{vmatrix}
        -1 & -\lambda\\
        -2 & -2
    \end{vmatrix}\\
    &=-\lambda\left(\lambda^2-2\right)+2\left(\lambda - 2\right)+2\left(2-2\lambda\right)\\
    &=-\lambda^3+2\lambda+2\lambda-4+4-4\lambda\\
    &=-\lambda^3\Rightarrow \lambda_1=\lambda_2=\lambda_3=0\\
    &\Rightarrow \rho(B_J)=|\lambda_1|=|\lambda_2|=|\lambda_3|=0\\
    C_{B_{GS}}(\lambda)&=
    \begin{vmatrix}
        -\lambda & -2 & 2\\
        0 & 2-\lambda & -3\\
        0 & 0 & 2-\lambda
    \end{vmatrix}
    =-\lambda
    \begin{vmatrix}
        2-\lambda & -3\\
        0 & 2-\lambda
    \end{vmatrix}
    =-\lambda\left(2-\lambda\right)\left(2-\lambda\right)\\
    &\Rightarrow \lambda_1=0,\indent\lambda_2=\lambda_3=2\\
    &\Rightarrow |\lambda_1|<|\lambda_2|=|\lambda_3|=2=\rho(B_{GS})
\end{align*}
\subsubsection*{iii)\normalfont Apply Jacobi and Gauss-Seidel to the system with $x^{(0)}={\bf0}$ and 25 iterations.
Comment on the findings. The exact solution is $x=[1$ $2$ $-1]^T$.}
Using the Matlab functions designed for problem 4, we attain the following results for the system $Ax=b$:
\begin{align*}
    \text{$myJacobi()$: }x^{(25)}=
    \begin{bmatrix}
        1\\
        2\\
        -1
    \end{bmatrix}
    ,\indent \text{$myGaussSeidel()$: }x^{(25)}=
    \begin{bmatrix}
        1.3086\times 10^9\\
        -1.3254\times 10^9\\
        0.0336\times 10^9
    \end{bmatrix}\\
    \text{$myJacobi()$: }\|x-x^{(25)}\|=0
    ,\indent\text{$myGaussSeidel()$: }\|x-x^{(25)}\|=1.8629\times 10^9
\end{align*}
This time, the Gauss-Seidel algorithm fails to converge for the system $Ax=b$ within 25 iterations while the Jacobi 
algorithm converges handily. Increasing the number of iterations does not improve the outcome for Gauss-Seidel:
\begin{align*}
    \text{$myGaussSeidel()$: }x^{(1000)}=
    \begin{bmatrix}
        1.6089\times10^{304}\\
        -1.6094\times10^{304}\\
        0.0011\times10^{304}
    \end{bmatrix}
\end{align*}
This result is again attributable to the respective spectral radii of $B_J$ and $B_{GS}$, where now 
$B_J=0<1$ and $B_{GS}=2>1$. By the same theorem as was used to argue on the convergence of these algorithms
in 5.a.iii, we would expect the Jacobi algorithm to converge for all choices of $x^{(0)}$ while the 
Gauss-Seidel algorithm is not guaranteed to converge for all $x^{(0)}$, but most certainly does not converge
for $x^{(0)}={\bf0}$.
\section*{References}
[1] James F. Epperson. {\it An introduction to numerical methods and analysis, second edition}. John Wiley \& Sons, Hoboken, New
Jersey, 2013.
\end{document}